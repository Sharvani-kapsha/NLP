# -*- coding: utf-8 -*-
"""projectofnlp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kRiawasRdT2MmTfOCHm9RzHOdr5ufS6L
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.offline as po
import matplotlib.pyplot as plt

import plotly.express as px
import random
import plotly.figure_factory as ff
from plotly import tools
from plotly.subplots import make_subplots
from plotly.offline import iplot
import warnings
warnings.filterwarnings("ignore")
pd.set_option("display.max_rows",None)
from sklearn import preprocessing
# %matplotlib inline

dataset=pd.read_csv("/content/project(2).csv")
dataset

dataset.info()

import string
import pandas as pd

def remove_punctuation(text):
    """Removes punctuation from a string."""
    # Convert non-string values to strings
    if isinstance(text, str):
        punctuation_free = "".join([i for i in text if i not in string.punctuation])
    else:
        punctuation_free = str(text)
    return punctuation_free

def clean_dataset(dataset):
    """Applies punctuation removal to the 'common symptoms' column."""
    dataset['clean_msg'] = dataset['common symptoms'].apply(remove_punctuation)
    return dataset

# Assuming you have Pandas installed and your dataset is loaded as `df`:
cleaned_dataset = clean_dataset(dataset.copy())  # Create a copy to avoid modifying the original
print(cleaned_dataset.head())

dataset['Knowledge:\nHow familiar are you with the term "hypothyroidism" ?'].isnull().sum()

def clean_dataset(dataset):
    """Applies punctuation removal and lowercasing to the 'common symptoms' column."""
    dataset['clean_msg'] = dataset['common symptoms'].apply(remove_punctuation)

    # Check the data type of each element in the 'common symptoms' column
    # and apply the lower() method only to strings
    dataset['msg_lower'] = dataset['common symptoms'].apply(lambda x: x.lower() if isinstance(x, str) else x)

    return dataset

# Assuming you have Pandas installed and your dataset is loaded as `df`:
cleaned_dataset = clean_dataset(dataset.copy())  # Create a copy to avoid modifying the original
print(cleaned_dataset.head())

cleaned_dataset = clean_dataset(dataset.copy())

def clean_dataset(dataset):
    """Applies punctuation removal and lowercasing to the 'common symptoms' column."""
    dataset['clean_msg'] = dataset['common symptoms'].apply(remove_punctuation)

    # Check the data type of each element in the 'common symptoms' column
    # and apply the lower() method only to strings
    dataset['msg_lower'] = dataset['common symptoms'].apply(lambda x: x.lower() if isinstance(x, str) else x)

    # Print the head of the 'msg_lower' column
    print(dataset['msg_lower'].head())

    return dataset

cleaned_dataset = clean_dataset(dataset.copy())

cleaned_dataset = clean_dataset(dataset.copy())

dataset.describe()

!pip install nltk

import nltk
nltk.download('punkt')

from nltk.tokenize import word_tokenize

def clean_dataset(dataset):
    # Convert the dataset to string if it's not already
    if not isinstance(dataset, str):
        dataset = str(dataset)

    # Your cleaning operations here
    # For example, tokenizing the dataset using NLTK
    tokenized_text = nltk.word_tokenize(dataset)

    # Returning the cleaned dataset
    return tokenized_text

# Example usage
dataset = "Your dataset string goes here."
cleaned_dataset = clean_dataset(dataset)

import pandas as pd

# Assuming 'dataset' is a DataFrame
if isinstance(dataset, pd.DataFrame) and not 'msg_lower' in dataset.columns:
    print("The 'clean_dataset' function was not executed or failed.")

def clean_dataset_original(dataset):
    """Original version of the 'clean_dataset' function."""
    # ... (same as the original function)

if clean_dataset != clean_dataset_original:
    print("The 'clean_dataset' function was modified.")

# Compare the outputs of the functions on a sample dataset
sample_dataset =pd.read_csv("/content/project(2).csv") # your sample dataset here

# Outputs of the original and modified functions
original_output = clean_dataset_original(sample_dataset)
modified_output = clean_dataset(sample_dataset)

# Compare the outputs
if original_output == modified_output:
    print("The outputs of the functions are identical.")
else:
    print("The outputs of the functions are different.")

# You can further log or analyze the differences in outputs

import pandas as pd

# Assuming dataset is a DataFrame
def clean_dataset(dataset):
    """Applies punctuation removal, lowercasing, and tokenization to the 'common symptoms' column."""
    dataset['clean_msg'] = dataset['common symptoms'].apply(remove_punctuation)

    # Convert all elements in the 'common symptoms' column to strings
    dataset['common symptoms'] = dataset['common symptoms'].astype(str)

    # Check the data type of each element in the 'common symptoms' column
    # and apply the lower() method only to strings
    dataset['msg_lower'] = dataset['common symptoms'].apply(lambda x: x.lower())

    # Tokenize the text in the 'common symptoms' column
    from nltk.tokenize import word_tokenize
    dataset['msg_tokenized'] = dataset['msg_lower'].apply(word_tokenize)

    # Print the head of the 'msg_tokenized' column
    print(dataset['msg_tokenized'].head())

    return dataset

# Example DataFrame creation
dataset = pd.DataFrame({'common symptoms': ['Symptom 1', 'Symptom 2', 'Symptom 3']})

# Call the clean_dataset function with the DataFrame copy
cleaned_dataset = clean_dataset(dataset.copy())

if 'no_stopwords' not in dataset.columns:
    print("The 'no_stopwords' column does not exist in the dataset.")

def remove_stopwords(text):
    # Implement the stop words removal logic here
    pass

if not callable(remove_stopwords):
    raise ValueError("The 'remove_stopwords' function is not defined.")

import pandas as pd

# Check the data type of each element in the 'common symptoms' column
dataset['common symptoms'] = dataset['common symptoms'].astype(str)

# Apply the remove_stopwords function to the 'common symptoms' column
dataset['no_stopwords'] = dataset['common symptoms'].apply(remove_stopwords)

def remove_stopwords(text):
    if isinstance(text, str):
        output = [i for i in text.split() if i not in stopwords]
        return output
    else:
        return text

for col in dataset.columns:
    if col.lower() == 'no_stopwords':
        print("The column name is misspelled. It should be '{}' instead of '{}'.".format(col, 'no_stopwords'))

import nltk
nltk.download('wordnet')

def apply_stemming(text):
    if isinstance(text, list):
        text = ' '.join(text)
    return ' '.join([stemmer.stem(word) for word in text.split()])

# Check for missing values in the 'no_stopwords' column
missing_values = dataset['no_stopwords'].isnull().sum()

if missing_values > 0:
    print("There are {} missing values in the 'no_stopwords' column.".format(missing_values))
    # Handle missing values (e.g., drop rows, impute values)

import pandas as pd
from nltk.stem import PorterStemmer

# Initialize the stemmer
stemmer = PorterStemmer()

# Function to apply stemming
def apply_stemming(text):
    if text is None:
        return ''
    if isinstance(text, list):
        text = ' '.join(text)
    return ' '.join([stemmer.stem(word) for word in text.split()])

# Example DataFrame creation
#dataset = pd.DataFrame({'no_stopwords': ['word1 word2 word3', None, 'word4 word5', 'word6']})

# Apply the stemming function to the 'no_stopwords' column
dataset['msg_stemmed'] = dataset['no_stopwords'].apply(apply_stemming)

# Check if the 'msg_stemmed' column exists in the DataFrame
if 'msg_stemmed' not in dataset.columns:
    # If the column does not exist, raise an error
    raise KeyError("The 'msg_stemmed' column does not exist in the DataFrame.")

# Print the first five rows of the 'msg_stemmed' column
print(dataset['msg_stemmed'].head())

# Commented out IPython magic to ensure Python compatibility.

import seaborn as sns

# %matplotlib inline
sns.set(rc={'figure.figsize': [8, 8]}, font_scale=1.2)

import matplotlib.pyplot as plt

# Split the text in the 'msg_stemmed' column into individual words
words = ' '.join(dataset['msg_stemmed']).split()

# Create a DataFrame to count the frequency of each stemmed word
word_counts = pd.Series(words).value_counts().reset_index()
word_counts.columns = ['Stemmed_Word', 'Frequency']

# Plot the bar chart
plt.figure(figsize=(8, 4))
sns.barplot(x='Stemmed_Word', y='Frequency', data=word_counts.head(10))  # Adjust head(10) for the number of words you want to display
plt.title('Top 10 Most Frequent Stemmed Words')
plt.xlabel('Stemmed Word')
plt.ylabel('Frequency')
plt.xticks(rotation=45)
plt.show()

# creating the dataset
data = {'No':20, 'Yes':15}
courses = list(data.keys())
values = list(data.values())

fig = plt.figure(figsize = (10, 5))

# creating the bar plot
plt.bar(courses, values, color ='maroon',
		width = 0.2)

plt.xlabel("Have you ever been diagnosed with hypothyroidism")
plt.ylabel("Age")
plt.title("prediction of KAP ")
plt.show()

sns.distplot(dataset['Gender'])

sns.jointplot(x='Age', y='Have you ever been diagnosed with hypothyroidism', data=dataset, kind='scatter', height=8, color='m')

"""# Check the column names of your DataFrame
print(dataset.columns)
sns.distplot(dataset['msg_stemmed'])
"""

dataset=dataset.replace({"Female":1,"Male":0})
dataset

dataset=dataset.replace({"Yes":1,"No":0})

dataset

"""print(dataset.columns)
# Assuming the correct column name is different than 'Gender', replace it with the actual column name
sns.distplot(dataset['msg_stemmed'])
"""

# Assuming 'dataset' is your existing DataFrame with columns 'no_stopwords' and 'msg_stemmed'

# Create a new DataFrame including the 'Gender' column
dataset_with_gender = pd.DataFrame({
    'no_stopwords': dataset['no_stopwords'],
    'msg_stemmed': dataset['msg_stemmed'],
    'Gender': ['Male', 'Female','Female']  # Replace this with your actual 'Gender' data
})

# Plot the distribution of the 'Gender' column
sns.countplot(data=dataset_with_gender, x='Gender')

# Check the column names of your DataFrame
print(dataset.columns)

dataset.rename(columns={'<Age>': 'msg_stemmed'}, inplace=True)

import seaborn as sns
import matplotlib.pyplot as plt

try:
    # Plot the distribution of the 'Age' column
    sns.distplot(dataset.loc[:, 'Age'])

    # Set plot labels and title
    plt.xlabel('Age')
    plt.ylabel('Density')
    plt.title('Distribution of Age')

    # Show the plot
    plt.show()
except KeyError as e:
    print(f"The 'Age' column does not exist in the DataFrame: {e}")
except Exception as e:
    print(f"An error occurred: {e}")

import pandas as pd

# Sample function to calculate age based on some criteria
def calculate_age(row):
  """Calculates age based on the length of the stemmed message"""
  # Check if 'msg_stemmed' exists, handle potential error otherwise
  if 'msg_stemmed' in row.index:
      age = len(row['msg_stemmed'])
  else:
      # Handle the case where 'msg_stemmed' is not present (e.g., assign default value, raise exception)
      age = 0  # Example: Assign 0 as age if the column is missing
      # raise ValueError("Column 'msg_stemmed' not found")  # Example: Raise an exception
  return age

# Assuming 'dataset' is your DataFrame after removing stopwords and stemming the message column
# and 'msg_stemmed' is the column containing the stemmed messages (or a different name)
dataset['Age'] = dataset.apply(calculate_age, axis=1)

# Now, 'age' column has been added to the DataFrame with calculated ages (or appropriate handling)

dataset['Age'] = dataset.apply(calculate_age, axis=1)

if 'Age' not in dataset.columns:
    raise ValueError("The 'dataset' DataFrame does not have an 'Age' column.")

# Check the column names of your DataFrame
print(dataset.columns)
dataset['common symptoms'] = pd.to_numeric(dataset['common symptoms'], errors='coerce')

import seaborn as sns
import matplotlib.pyplot as plt

# Group the data by 'common symptoms'
grouped_data = dataset.groupby('common symptoms')

# Create individual boxplots for each group
for name, group in grouped_data:
    print(f"Group: {name}, Count: {len(group)}")  # Check number of data points in each group
    sns.boxplot(x='age', data=group, label=name)

# Add labels and legend
plt.xlabel('Age')
plt.ylabel('Common Symptoms')
plt.legend(title='Common Symptoms')

# Show the plot
plt.show()

"""Below comming four code lines are same which represents about the gender and non numeric values in Gender column"""

# Load your data into a DataFrame (replace 'your_data.csv' with the actual filename)
dataset = pd.read_csv('project(2).csv')

# Check the column names for any leading or trailing whitespace
dataset.columns = dataset.columns.str.strip()

# Double-check the spelling of the column name
if 'Gender' in dataset.columns:
    # Access the 'Gender' column
    gender_column = dataset['Gender']
    print("First 5 values of 'Gender' column:")
    print(gender_column.head())
else:
    print("'Gender' column not found in the DataFrame. Double-check the column name spelling.")

import pandas as pd

# Check the data type of the 'Gender' column
print("Data type of 'Gender' column:", dataset['Gender'].dtype)

# Remove leading or trailing whitespace from column names
dataset.columns = dataset.columns.str.strip()

# Verify the column name spelling
if 'Gender' in dataset.columns:
    print("'Gender' column is present in the dataset.")
else:
    print("'Gender' column is not present in the dataset.")

# Check for missing values in the 'Gender' column
missing_values = dataset['Gender'].isnull().sum()
print("Number of missing values in 'Gender' column:", missing_values)

# Access the 'Gender' column using different indexing methods
print("First 5 values of 'Gender' column:")
print(dataset['Gender'].head())  # Using column name indexing
print(dataset.iloc[:, dataset.columns.get_loc('Gender')].head())  # Using column index

import pandas as pd

# Check if 'Gender' is present in the dataset
if 'Gender' in dataset.columns:
    # Check for non-numeric values in the 'Gender' column
    non_numeric_values = dataset['Gender'].apply(lambda x: not pd.to_numeric(x, errors='ignore'))

    # Print the non-numeric values
    print(dataset['Gender'][non_numeric_values])
else:
    print("The 'Gender' column is not present in the dataset.")

import pandas as pd

# Check for non-numeric values in the 'Gender' column
non_numeric_values = dataset['Gender'].apply(lambda x: not pd.to_numeric(x, errors='ignore'))

# Print the non-numeric values
print(dataset['Gender'][non_numeric_values])

dataset = dataset.dropna(subset=['Gender'])

"""below added new code to fill the missing values"""

# Fill missing values in the 'Gender' column with the most frequent value
dataset['Gender'] = dataset['Gender'].fillna(dataset['Gender'].mode()[0])

# Print the shape of the 'Gender' column data
print("Shape of 'Gender' column data:", dataset['Gender'].shape)

# Print the first few rows of the 'Gender' column
print("First few rows of 'Gender' column:\n", dataset['Gender'].head())

"""so here instead of imputers we use the fillna mode() This approach bypasses the need for the SimpleImputer and directly handles missing values within the DataFrame"""

# Fill missing values in the 'Gender' column with the most frequent value
dataset['Gender'] = dataset['Gender'].fillna(dataset['Gender'].mode()[0])

from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
dataset['Gender'] = encoder.fit_transform(dataset['Gender'])

import pandas as pd

# Check for non-numeric values in the 'Gender' column
non_numeric_values = dataset['Gender'].apply(lambda x: not pd.to_numeric(x, errors='ignore'))

# Drop rows with non-numeric values
dataset = dataset.drop(dataset.index[non_numeric_values])

from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
dataset['Gender'] = encoder.fit_transform(dataset['Gender'])

import pandas as pd

# Assuming `dataset` is your DataFrame

# Select only the numeric columns in the DataFrame
numeric_dataset = dataset.select_dtypes(include='number')

# Compute the correlation matrix for the numeric DataFrame
dataset_corr = numeric_dataset.corr()

# Print the correlation matrix
print(dataset_corr)

sns.heatmap(dataset_corr, cmap='viridis', linecolor='k', linewidths=2, annot=True)

dataset.dropna()

import matplotlib.pyplot as plt

# Assuming you have a dataset 'data' containing hypothyroidism-related features and KAP scores
# Example data format: data = [(tsh_level_1, kap_score_1), (tsh_level_2, kap_score_2), ...]

# Extracting hypothyroidism-related feature (TSH levels) and KAP scores from the dataset
tsh_levels = [entry[0] for entry in dataset]
kap_scores = [entry[1] for entry in dataset]

# Creating the scatter plot
plt.figure(figsize=(8, 6))
plt.scatter(tsh_levels, kap_scores, color='blue', alpha=0.5)
plt.title('Scatter Plot of TSH Levels vs KAP Scores')
plt.xlabel('TSH Levels')
plt.ylabel('KAP Scores')
plt.grid(True)
plt.show()

dataset.columns

dataset.dtypes

dataset.isnull().sum()

import torch
from torch.utils.data import DataLoader, TensorDataset
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load pre-trained BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Define your dataset
texts = []
labels = [0, 1, ...] # Numerical labels corresponding to each text

# Check if the text is a string before tokenizing
tokenized_texts = []
for text in texts:
    if isinstance(text, str):
        tokenized_texts.append(tokenizer.tokenize(text))
    else:
        print(f"Skipping non-string text: {text}")

# Now tokenized_texts contains only tokenized texts from the valid strings in texts

# Convert tokens to token IDs
input_ids = [tokenizer.convert_tokens_to_ids(tokens) for tokens in tokenized_texts]

import torch
from transformers import BertTokenizer

# Load pre-trained BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Define your dataset
texts = ["Age", "Gender", "Knowledge"]
labels = [0, 1, 0]  # Numerical labels corresponding to each text

# Check if the text is a string before tokenizing
tokenized_texts = []
for text in texts:
    if isinstance(text, str):
        tokenized_texts.append(tokenizer.tokenize(text))
    else:
        print(f"Skipping non-string text: {text}")

# Filter out empty tokenized texts
tokenized_texts = [tokens for tokens in tokenized_texts if tokens]

# Convert tokens to token IDs
input_ids = [tokenizer.convert_tokens_to_ids(tokens) for tokens in tokenized_texts]

# Pad sequences to the same length if the input_ids list is not empty
if input_ids:
    max_length = max(len(ids) for ids in input_ids)
    padded_input_ids = [ids + [tokenizer.pad_token_id] * (max_length - len(ids)) for ids in input_ids]
else:
    padded_input_ids = []

# Convert labels to PyTorch tensor
labels_tensor = torch.tensor(labels, dtype=torch.int64)

# Convert padded_input_ids to PyTorch tensor
input_ids_tensor = torch.tensor(padded_input_ids, dtype=torch.int64)  # Assuming input IDs are integers

# Now you can use input_ids_tensor and labels_tensor as needed

# Pad sequences to the same length
max_length = max(len(ids) for ids in input_ids)
padded_input_ids = [ids + [tokenizer.pad_token_id] * (max_length - len(ids)) for ids in input_ids]

import torch

# Clean the labels list
cleaned_labels = []

for label in labels:
    # Check if the label is a valid number (int or float)
    if isinstance(label, (int, float)):
        cleaned_labels.append(label)
    else:
        print(f"Skipping invalid value: {label}")

# Convert the cleaned list to a PyTorch tensor
labels_tensor = torch.tensor(cleaned_labels, dtype=torch.int64)  # Use the appropriate dtype for your case

# Convert padded_input_ids to a PyTorch tensor
input_ids_tensor = torch.tensor(padded_input_ids, dtype=torch.int64)  # Assuming input IDs are integers

# Now you can use input_ids_tensor and labels_tensor as needed

# Create attention masks
attention_masks = [[int(token_id != tokenizer.pad_token_id) for token_id in ids] for ids in input_ids]

# Convert attention_masks to a tensor
train_masks_tensor = torch.tensor(attention_masks)

import torch

# Create a sample tensor
tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])

# Check the size of the tensor
print("Tensor size:", tensor.size())

import torch

# Define some example tensors
train_inputs = torch.randn(10, 5)  # Example tensor with shape (10, 5)
train_masks = torch.ones(10, 5)    # Example tensor with shape (10, 5)
train_labels = torch.zeros(10)     # Example tensor with shape (10,)

# Check the sizes of the tensors
print("Train inputs size:", train_inputs.size())
print("Train masks size:", train_masks.size())
print("Train labels size:", train_labels.size())

#import torch
#
## Assuming train_inputs, train_masks, and train_labels are defined elsewhere
#
## Check the sizes of the tensors
#print("Train inputs size:", train_inputs.size(0))
#print("Train masks size:", train_masks.size(0))
#print("Train labels size:", train_labels.size(0))

print("Train inputs size:", train_inputs.size(0))
print("Train masks size:", train_masks.size(0))
print("Train labels size:", train_labels.size(0))

from torch.utils.data import TensorDataset

# Convert lists to PyTorch tensors
train_inputs_tensor = torch.tensor(train_inputs)
train_labels_tensor = torch.tensor(train_labels)

# Adjust train_masks_tensor to match the number of samples in train_inputs_tensor and train_labels_tensor
train_masks_tensor = torch.tensor(train_masks[:len(train_inputs)])

# Create a TensorDataset for training data
train_data = TensorDataset(train_inputs_tensor, train_masks_tensor, train_labels_tensor)

# Split data into training and validation sets
train_inputs, val_inputs, train_labels, val_labels, train_masks, val_masks = train_test_split(
input_ids_tensor, labels_tensor, attention_masks, random_state=42, test_size=0.1
)

print(f"Size of train_labels: {train_labels.size()}")

train_labels = train_labels.expand(train_inputs.size(0))

train_labels = torch.zeros(train_inputs.size(0), dtype=torch.int64)

train_labels = torch.zeros(train_inputs.size(0), dtype=torch.long)

import torch
from torch.utils.data import TensorDataset

# Convert lists to PyTorch tensors
print("Train inputs shape:", train_inputs_tensor.shape)
print("Train masks shape:", train_masks_tensor.shape)
print("Train labels shape:", train_labels_tensor.shape)


# Create a TensorDataset for training data
train_data = TensorDataset(train_inputs_tensor, train_masks_tensor, train_labels_tensor)

# Load pre-trained BERT model
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# Define your dataset
texts = ["Age", "Gender"]
labels = [0, 1]  # Numerical labels corresponding to each text

# Set device (GPU if available)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Set optimizer and learning rate scheduler
optimizer = AdamW(model.parameters(), lr=2e-5)

from torch.utils.data import DataLoader, Dataset

# Define your custom dataset class
class CustomDataset(Dataset):
    def __init__(self, data):
        # Initialize your dataset here
        self.data = data

    def __len__(self):
        # Return the total number of samples in your dataset
        return len(self.data)

    def __getitem__(self, idx):
        # Implement logic to retrieve a sample from your dataset
        # Return the sample data and its corresponding label
        return self.data[idx]

# Initialize your train_dataset
train_dataset = CustomDataset(dataset)  # Replace project_data with your actual dataset

# Define other necessary parameters
batch_size = 32
shuffle = True

# Initialize your train_dataloader
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)

import torch
from torch.utils.data import DataLoader, TensorDataset
from transformers import AutoModelForSequenceClassification, AutoTokenizer

# Initialize tokenizer and model
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased')

# Example data (replace with your actual data)
train_texts = ['Age','Gender', 'Knowledge']
train_labels = [0, 1, 0]  # Example labels

# Tokenize input texts and convert labels to tensors
tokenized_inputs = tokenizer(train_texts, padding=True, truncation=True, return_tensors="pt")
train_labels_tensor = torch.tensor(train_labels)

# Create a TensorDataset for training data
train_dataset = TensorDataset(tokenized_inputs["input_ids"], tokenized_inputs["attention_mask"], train_labels_tensor)

# Initialize DataLoader for training set
batch_size = 32  # Adjust the batch size as needed
shuffle = True
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)

# Example training loop
num_epochs = 8  # Set the number of epochs
for epoch in range(num_epochs):
    total_loss = 0
    for batch in train_dataloader:
        batch = tuple(t.to(device) for t in batch)
        inputs, masks, labels = batch
        optimizer.zero_grad()
        outputs = model(inputs, attention_mask=masks, labels=labels)
        loss = outputs.loss
        total_loss += loss.item()  # Add the current loss to the total loss
        loss.backward()
        optimizer.step()

    avg_train_loss = total_loss / len(train_dataloader)
    print(f'Epoch {epoch+1}/{num_epochs}, Average Training Loss: {avg_train_loss}')

from torch.utils.data import DataLoader

# Assuming you have a dataset called train_dataset
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# Now you can use train_dataloader in your loop
total_loss = 0

for batch in train_dataloader:
    batch = tuple(t.to(device) for t in batch)
    inputs, masks, labels = batch
    optimizer.zero_grad()
    outputs = model(inputs, attention_mask=masks, labels=labels)
    loss = outputs.loss
    total_loss += loss.item()  # Add the current loss to the total loss
    loss.backward()
    optimizer.step()
    avg_train_loss = total_loss / len(train_dataloader)
    print(f'Epoch {epoch+1}/{num_epochs}, Average Training Loss: {avg_train_loss}')

# Initialize total_loss before the loop
total_loss = 0

for batch in train_dataloader:
    batch = tuple(t.to(device) for t in batch)
    inputs, masks, labels = batch
    optimizer.zero_grad()
    outputs = model(inputs, attention_mask=masks, labels=labels)
    loss = outputs.loss
    total_loss += loss.item()  # Add the current loss to the total loss
    loss.backward()
    optimizer.step()
    avg_train_loss = total_loss / len(train_dataloader)
    print(f'Epoch {epoch+1}/{num_epochs}, Average Training Loss: {avg_train_loss}')

from torch.utils.data import DataLoader, TensorDataset

# Assuming you have PyTorch tensors for validation data (val_inputs, val_masks, val_labels)
# and a batch size (batch_size)
val_inputs_tensor = torch.tensor(val_inputs)
val_masks_tensor = torch.tensor(val_masks)
val_labels_tensor = torch.tensor(val_labels)

# Create a TensorDataset with the validation data tensors
val_dataset = TensorDataset(val_inputs_tensor, val_masks_tensor, val_labels_tensor)

# Create a DataLoader with the validation dataset
# Set batch size as needed (e.g., batch_size = 32)
val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)

# Validation
model.eval()
val_preds = []
val_true = []

# Iterate through the validation DataLoader
for batch in val_dataloader:
    # Ensure proper indentation for the code block
    batch = tuple(t.to(device) for t in batch)
    inputs, masks, labels = batch

    with torch.no_grad():
        outputs = model(inputs, attention_mask=masks)
        logits = outputs.logits
        preds = torch.argmax(logits, dim=1).detach().cpu().numpy()

    # Extend the predictions and true labels lists
    val_preds.extend(preds)
    val_true.extend(labels.cpu().numpy())

val_accuracy = accuracy_score(val_true, val_preds)
print(f'Validation Accuracy: {val_accuracy}')

# Save the fine-tuned model
model.save_pretrained('fine_tuned_bert_model')

from torch.utils.data import TensorDataset, DataLoader
# Assuming train_inputs, train_masks, and train_labels are lists
train_inputs = torch.tensor(train_inputs)
train_masks = torch.tensor(train_masks)
train_labels = torch.tensor(train_labels)

print("Train inputs size:", train_inputs_tensor.size)

print("Train inputs size:", train_inputs.size())
print("Train masks size:", train_masks.size())
print("Train labels size:", train_labels.size())

import torch
import torch.utils.data

# Assuming train_inputs, train_masks, and train_labels are properly defined

# Adjust the size of train_labels to match train_inputs
train_labels = train_labels[:train_inputs.size(0)]

# Check the sizes of the tensors after adjustment
print("Train inputs size:", train_inputs.size(0))
print("Train masks size:", train_masks.size(0))
print("Train labels size:", train_labels.size(0))

# Ensure that the number of samples in inputs and labels match
assert train_inputs.size(0) == train_labels.size(0), "Number of inputs and labels should match"

# Create DataLoader for the training set
train_data = torch.utils.data.TensorDataset(train_inputs, train_masks, train_labels)
train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)

# Create DataLoader for training and validation sets
train_data = TensorDataset(train_inputs, train_masks, train_labels)
train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True)

import torch
from torch.utils.data import TensorDataset, DataLoader
# Assuming val_inputs, val_masks, and val_labels are lists
val_inputs = torch.tensor(val_inputs)
val_masks = torch.tensor(val_masks)
val_labels = torch.tensor(val_labels)

# Create DataLoader for validation set
val_data = TensorDataset(val_inputs, val_masks, val_labels)
val_dataloader = DataLoader(val_data, batch_size=32)

import pandas as pd

df = pd.read_csv("/content/project(2).csv")

df.head()

texts = []
labels = []  # Example labels

df["Have you ever been diagnosed with hypothyroidism"].head()

for i in df["common symptoms"]:
  texts.append(i)

for i in df["Have you ever been diagnosed with hypothyroidism"]:
  if i.lower()=="Somewhat familiar".lower():
    labels.append(1)
  else:
    labels.append(0)


print(texts)
print(labels)

df.dropna(subset=['common symptoms'], inplace=True)

# Extract texts and labels from the DataFrame
texts = df['common symptoms'].tolist()

# Load pre-trained BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokenize and encode the texts
tokenized_texts = [tokenizer.encode(text, add_special_tokens=True) for text in texts]

# Pad sequences to the same length
max_length = max(len(ids) for ids in tokenized_texts)
padded_tokenized_texts = [ids + [tokenizer.pad_token_id] * (max_length - len(ids)) for ids in tokenized_texts]



# Convert lists to PyTorch tensors
input_ids = torch.tensor(padded_tokenized_texts)
labels_tensor = torch.tensor(labels)

# Load pre-trained BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokenize and encode the texts
tokenized_texts = [tokenizer.encode(text, add_special_tokens=True) for text in texts]

# Pad sequences to the same length
max_length = max(len(ids) for ids in tokenized_texts)
padded_tokenized_texts = [ids + [tokenizer.pad_token_id] * (max_length - len(ids)) for ids in tokenized_texts]

# Convert lists to PyTorch tensors
input_ids = torch.tensor(padded_tokenized_texts)
labels_tensor = torch.tensor(labels)

# Create attention masks
attention_masks = [[int(token_id != tokenizer.pad_token_id) for token_id in ids] for ids in padded_tokenized_texts]
attention_masks_tensor = torch.tensor(attention_masks)

# Define training and validation datasets
#train_inputs, val_inputs, train_labels, val_labels = train_test_split(input_ids, labels_tensor, test_size=0.2, random_state=42)
#train_masks, val_masks, _, _ = train_test_split(attention_masks_tensor, input_ids, test_size=0.2, random_state=42)

# Define DataLoader for training and validation sets
train_dataset = TensorDataset(train_inputs, train_masks, train_labels)
val_dataset = TensorDataset(val_inputs, val_masks, val_labels)

train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)

# Define the model architecture
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# Define optimizer and learning rate scheduler
optimizer = AdamW(model.parameters(), lr=2e-5)

# Training loop
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

num_epochs = 9
for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    for batch in train_dataloader:
        batch = tuple(t.to(device) for t in batch)
        inputs, masks, labels = batch
        optimizer.zero_grad()
        outputs = model(inputs, attention_mask=masks, labels=labels)
        loss = outputs.loss
        total_loss += loss.item()
        loss.backward()
        optimizer.step()
    avg_train_loss = total_loss / len(train_dataloader)
    print(f'Epoch {epoch + 1}/{num_epochs}, Average Training Loss: {avg_train_loss}')

# Validation loop
model.eval()
val_preds = []
val_true = []
for batch in val_dataloader:
    batch = tuple(t.to(device) for t in batch)
    inputs, masks, labels = batch
    with torch.no_grad():
        outputs = model(inputs, attention_mask=masks)
    logits = outputs.logits
    preds = torch.argmax(logits, dim=1).tolist()
    val_preds.extend(preds)
    val_true.extend(labels.tolist())

# Calculate validation accuracy
val_accuracy = accuracy_score(val_true, val_preds)
print(f'Validation Accuracy: {val_accuracy}')

import torch

# Assuming padded_input_ids, attention_masks_tensor, and labels_tensor are lists

# Convert lists to PyTorch tensors
padded_input_ids = torch.tensor(padded_input_ids)
attention_masks_tensor = torch.tensor(attention_masks_tensor)
labels_tensor = torch.tensor(labels_tensor)

# Print the sizes of the tensors
print("Size of padded_input_ids:", padded_input_ids.size())
print("Size of attention_masks_tensor:", attention_masks_tensor.size())
print("Size of labels_tensor:", labels_tensor.size())

# Assuming labels is a list of labels
labels_tensor = torch.tensor(labels)

print("Size of padded_input_ids:", padded_input_ids.size())
print("Size of attention_masks_tensor:", attention_masks_tensor.size())
print("Size of labels_tensor:", labels_tensor.size())

